The idea is to have a song progress and show you what notes to hit, alongside visual feedback about what note you're actually hitting. So that you can learn to improve your ability to sing the song, do arpeggios, etc. I'm imagining that both the target note and the feedback are both conveyed in terms of sheet music, since that will have the added benefit of anchoring your learning to a conventional music format.

In terms of implementation, p5js lets you analyze the FFT and extract the energy in different pitch/frequency bands. But if the target is a middle C, say, it's not like you want to match your voice to a pure middle C sine wave. So what I was thinking was that you calibrate by having the singer sing a bunch of different notes (but with the same word, in case this matters), ranging from like the lowest to the highest. That will be a 1D manifold (corresponding to varying pitches) embedded in the high-D FFT feature space. For a given target pitch, you can then (presumably) find the point in high-D FFT feature space that best matches the target note. And then _that_ FFT feature space is your target vector.

To give visual feedback about the note being sung, you could similarly find the pitch whose embedding (in your calibration data) best matches the current note.

Would possibly be interesting to provide feedback about variance as well. Or to identify if there are times when you are consistent not hitting the target note as well (e.g., transitions between notes, or at the end of a held note). This could be part of an end-of-session summary, to quantify where the errors are in the song. Like, you sing a line of the song, maybe over and over again, and after a few trials of that, it highlights the parts of the sheet music where your error is largest.
